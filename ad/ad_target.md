# 基于 wide&deep 模型的智能召回服务
## 前言
粉丝头条广告支持多路召回，包括索引召回、向量召回、智能召回。
- 索引召回：利用倒排索引实现的定向召回，只针对设置了基础定向的订单。
- 向量召回：利用 DSSM 模型训练出用户向量和广告向量，线上通过计算二者内积得到相似度。
- 智能召回：利用 Wide&Deep 模型实现智能优选订单召回。

## Wide&Deep 简介
Wide&Deep 是 LR 和 DNN 两个模型的融合，使得它既有 LR 的记忆能力，又有 DNN 的泛化能力。

适用于输入非常稀疏的大规模分类或回归问题，比如推荐系统、search、ranking 问题，输入稀疏通常是由离散特征有非常非常多个可能的取值造成的，one-hot之后维度非常大。

### Wide
对应 LR 模型，通过 one-hot 编码构建离散特征，训练出每个特征的权重，由于是线性模型，具备很强的记忆性和可解释性。

LR 需要通过特征交叉来提高模型的表达能力，增加了特征工程的工作量。

LR 模型学习不到从未出现过的特征组合，不具备泛化能力。

### Deep
对应 DNN 模型，可以为每个特征学习到一个低维稠密向量（embedding），减轻了特征工程的负担。

学习的特征向量包含了任意特征之间的远近关系，包括训练样本中从未出现过的特征组合，这样模型就具备了泛化推理能力。

缺点是当 embedding 矩阵稀疏且高秩时，很难非常效率的学习出低维度的表示（如 user 有特殊的爱好或 item 比较小众），从而可能过度泛化，给出完全不相关的推荐，准确性不能得到保证。

### 联合训练
将 LR 和 DNN 联合训练，二者的输出通过加权方式合并到一起，并通过 logistic loss function 进行最终输出。

在训练的时候，根据最终的 loss 计算出梯度，反向传播到 Wide 和 Deep 两部分中，分别训练自己的参数。

联合训练下 Wide 和 Deep 的 size 都减小了，Wide 只需要填补 Deep 的不足就行，只需要较少的交叉特征，不需要全部。

### 优化器
Wide 部分是用 FTRL（Follow-the-regularized-leader） + L1 正则化学习。

Deep组件是用 AdaGrad 来学习。

## 模型训练
### 模型训练流程图
![train](images/train.png)

